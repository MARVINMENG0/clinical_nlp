{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n",
        "!pip install trl\n",
        "!pip install datasets\n",
        "!pip install bitsandbytes\n",
        "!pip install evaluate"
      ],
      "metadata": {
        "id": "7S1QyNPRV3bG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import gc\n",
        "from datasets import Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModel,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoModelForMaskedLM,\n",
        "    AutoModelForPreTraining,\n",
        "    AutoModelForCausalLM,\n",
        "    AutoModelForNextSentencePrediction,\n",
        "    DataCollatorWithPadding,\n",
        "    AutoTokenizer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "\n",
        "from huggingface_hub import login\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import bitsandbytes as bnb\n",
        "import evaluate\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "M0NFw_kvU0P6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Tokenize the datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\", use_4bit=True)\n",
        "max_seq_length = 128\n",
        "def tokenize_data(data):\n",
        "    return tokenizer(data['text'], truncation=True, padding='max_length', max_length=max_seq_length)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    precision = evaluate.load(\"precision\")\n",
        "    recall = evaluate.load(\"recall\")\n",
        "    f1 = evaluate.load(\"f1\")\n",
        "    accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\"precision\": precision.compute(predictions = predictions, references = labels)['precision'],\n",
        "            \"recall\": recall.compute(predictions = predictions, references = labels)['recall'],\n",
        "            \"accuracy\": accuracy.compute(predictions = predictions, references = labels)['accuracy'],\n",
        "            \"f1\": f1.compute(predictions = predictions, references = labels)['f1']}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Create a custom data collator for MLM and NSP\n",
        "class DataCollatorForPreTraining:\n",
        "    def __init__(self, tokenizer, mlm=True, mlm_probability=0.15):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.mlm = mlm\n",
        "        self.mlm_probability = mlm_probability\n",
        "\n",
        "    def __call__(self, examples):\n",
        "        batch = self.tokenizer.pad(examples, return_tensors=\"pt\")\n",
        "        if self.mlm:\n",
        "            inputs, labels = self.mask_tokens(batch[\"input_ids\"])\n",
        "            batch[\"input_ids\"] = inputs\n",
        "            batch[\"labels\"] = labels\n",
        "        else:\n",
        "            batch[\"labels\"] = batch[\"input_ids\"]\n",
        "\n",
        "        # Add dummy next sentence prediction labels (since we do not have sentence pairs)\n",
        "        batch[\"next_sentence_label\"] = torch.zeros(len(batch[\"input_ids\"]), dtype=torch.long)\n",
        "        return batch\n",
        "\n",
        "    def mask_tokens(self, inputs):\n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "        \"\"\"\n",
        "        labels = inputs.clone()\n",
        "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        return inputs, labels"
      ],
      "metadata": {
        "id": "Hbz-t-mcljCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Load in Data\n",
        "train_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/layer_health_data/train_data.csv\")\n",
        "\n",
        "### Split Labelled and Unlabelled\n",
        "train_data_labelled = train_data[~train_data[\"has_cancer\"].isnull()]\n",
        "train_data_unlabelled = train_data[train_data[\"has_cancer\"].isnull()]\n",
        "\n",
        "train_data_labelled['has_cancer'] = train_data_labelled['has_cancer'].astype(int)\n",
        "train_data_labelled['has_diabetes'] = train_data_labelled['has_diabetes'].astype(int)\n",
        "\n",
        "### Combine Cancer and Diabetes Labels\n",
        "train_data_labelled['label'] = train_data_labelled.apply(lambda row: [row['has_cancer'], row['has_diabetes']], axis=1)\n",
        "train_data_labelled = train_data_labelled.drop(['test_set', 'patient_identifier'], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Create Training and Validation sets\n",
        "train_split_labelled, test_split_labelled = train_test_split(train_data_labelled, test_size=0.3, random_state = 42)\n",
        "\n",
        "\n",
        "### Convert data to Huggingface Datasets\n",
        "labelled_dataset = Dataset.from_pandas(train_split_labelled)\n",
        "unlabelled_dataset = Dataset.from_pandas(train_data_unlabelled)\n",
        "labelled_eval_dataset = Dataset.from_pandas(test_split_labelled)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "tokenized_labelled = labelled_dataset.map(tokenize_data, batched=True)\n",
        "tokenized_labelled_eval = labelled_eval_dataset.map(tokenize_data, batched=True)\n",
        "\n",
        "tokenized_labelled_all = tokenized_labelled.select_columns(['label', 'input_ids', 'attention_mask'])\n",
        "tokenized_labelled_all.set_format(\"torch\")\n",
        "\n",
        "tokenized_labelled_eval_all = tokenized_labelled_eval.select_columns(['label','input_ids', 'attention_mask'])\n",
        "tokenized_labelled_eval_all.set_format(\"torch\")\n",
        "\n",
        "\n",
        "tokenized_labelled_cancer = tokenized_labelled.select_columns(['has_cancer', 'input_ids', 'attention_mask'])\n",
        "tokenized_labelled_cancer = tokenized_labelled_cancer.rename_column(\"has_cancer\", \"labels\")\n",
        "tokenized_labelled_cancer.set_format(\"torch\")\n",
        "\n",
        "tokenized_labelled_eval_cancer = tokenized_labelled_eval.select_columns(['has_cancer','input_ids', 'attention_mask'])\n",
        "tokenized_labelled_eval_cancer = tokenized_labelled_eval_cancer.rename_column(\"has_cancer\", \"labels\")\n",
        "tokenized_labelled_eval_cancer.set_format(\"torch\")\n",
        "\n",
        "tokenized_labelled_diabetes = tokenized_labelled.select_columns(['has_diabetes', 'input_ids', 'attention_mask'])\n",
        "tokenized_labelled_diabetes = tokenized_labelled_diabetes.rename_column(\"has_diabetes\", \"labels\")\n",
        "tokenized_labelled_diabetes.set_format(\"torch\")\n",
        "\n",
        "tokenized_labelled_eval_diabetes = tokenized_labelled_eval.select_columns(['has_diabetes','input_ids', 'attention_mask'])\n",
        "tokenized_labelled_eval_diabetes = tokenized_labelled_eval_diabetes.rename_column(\"has_diabetes\", \"labels\")\n",
        "tokenized_labelled_eval_diabetes.set_format(\"torch\")\n",
        "\n",
        "\n",
        "tokenized_unlabelled = unlabelled_dataset.map(tokenize_data, batched=True)\n",
        "tokenized_unlabelled = tokenized_unlabelled.select_columns(['input_ids', 'attention_mask'])\n",
        "tokenized_unlabelled.set_format(\"torch\")"
      ],
      "metadata": {
        "id": "C5jhHywtRwJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrainModel = AutoModelForPreTraining.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "maskedLMmodel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "cancerModel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "diabetesModel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "peft_config = LoraConfig(r = 16,\n",
        "    target_modules = [\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_rslora = True,)\n",
        "\n",
        "pretrainModel = get_peft_model(pretrainModel, peft_config)\n",
        "maskedLMmodel = get_peft_model(maskedLMmodel, peft_config)\n",
        "cancerModel = get_peft_model(cancerModel, peft_config)\n",
        "diabetesModel = get_peft_model(diabetesModel, peft_config)\n",
        "\n",
        "\n",
        "# pretrain_data_collator = DataCollatorForPreTraining(tokenizer)\n",
        "# mlm_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "vcqLgUPDLH4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class TrainArgs:\n",
        "    per_device_train_batch_size: int = 2\n",
        "    gradient_accumulation_steps: int = 4\n",
        "    warmup_steps: int = 5\n",
        "    num_train_epochs:int = 1\n",
        "    learning_rate: float = 1e-4\n",
        "    fp16: bool = False #not is_bfloat16_supported(),\n",
        "    bf16: bool = True #is_bfloat16_supported(),\n",
        "    logging_steps: int = 1\n",
        "    optim: str = \"adamw_8bit\"\n",
        "    weight_decay: float = 0.01\n",
        "    lr_scheduler_type: str = \"linear\"\n",
        "    seed: int = 890\n",
        "    output_dir: str = \"outputs\"\n",
        "args = TrainArgs()"
      ],
      "metadata": {
        "id": "0uhgvg781TGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Train"
      ],
      "metadata": {
        "id": "rs5YBYLJX5eS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "        model = pretrainModel,\n",
        "        tokenizer = tokenizer,\n",
        "        train_dataset = tokenized_unlabelled,\n",
        "        dataset_text_field = \"text\",\n",
        "        max_seq_length = 512,\n",
        "        dataset_num_proc = 2,\n",
        "        peft_config = peft_config,\n",
        "        packing = False,\n",
        "        data_collator=pretrain_data_collator,\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = TrainArgs.per_device_train_batch_size,\n",
        "            gradient_accumulation_steps = TrainArgs.gradient_accumulation_steps,\n",
        "            warmup_steps = TrainArgs.warmup_steps,\n",
        "            num_train_epochs = TrainArgs.num_train_epochs,\n",
        "            learning_rate = TrainArgs.learning_rate,\n",
        "            fp16 = TrainArgs.fp16,\n",
        "            bf16 = TrainArgs.bf16,\n",
        "            logging_steps = TrainArgs.logging_steps,\n",
        "            optim = TrainArgs.optim,\n",
        "            weight_decay = TrainArgs.weight_decay,\n",
        "            lr_scheduler_type = TrainArgs.lr_scheduler_type,\n",
        "            seed = TrainArgs.seed,\n",
        "            output_dir = TrainArgs.output_dir,\n",
        "        ),\n",
        "    )"
      ],
      "metadata": {
        "id": "HWWrQPhjGMnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "t2ZrYv8Lsz2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_file = \"drive/MyDrive/Colab Notebooks/layer_health_data/mlmHead.pth\"\n",
        "torch.save(maskedLMmodel.state_dict(), output_file)"
      ],
      "metadata": {
        "id": "ZwHdPiw-5FnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save locally with torch\n",
        "# output_file = \"drive/MyDrive/Colab Notebooks/layer_health_data/maskedLMModel.pth\"\n",
        "# torch.save(maskedLMmodel.state_dict(), output_file)\n",
        "\n",
        "\n",
        "# blankModel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "# peft_config = LoraConfig(r = 16,\n",
        "#     target_modules = [\"query\", \"key\", \"value\", \"dense\"],\n",
        "#     lora_alpha = 16,\n",
        "#     lora_dropout = 0,\n",
        "#     bias = \"none\",\n",
        "#     use_rslora = True,)\n",
        "\n",
        "# blankModel = get_peft_model(blankModel, peft_config)\n",
        "# blankModel.load_state_dict(torch.load(output_file))"
      ],
      "metadata": {
        "id": "C30-nFbL7S69"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tune"
      ],
      "metadata": {
        "id": "WgwktkCrX8L5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretrainModel.load_state_dict(torch.load(\"drive/MyDrive/Colab Notebooks/layer_health_data/pretrainHead.pth\"))\n",
        "maskedLMmodel.load_state_dict(torch.load(\"drive/MyDrive/Colab Notebooks/layer_health_data/mlmHead.pth\"))"
      ],
      "metadata": {
        "id": "T5H_4wZlX9dr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ### Transfer weights\n",
        "# cancerModel.bert = maskedLMmodel.bert # pretrainModel.bert\n",
        "# diabetesModel.bert = maskedLMmodel.bert # pretrainModel.bert\n",
        "\n",
        "cancerModel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "diabetesModel = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
        "\n",
        "peft_config = LoraConfig(r = 16,\n",
        "    target_modules = [\"classifier\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_rslora = True,)\n",
        "\n",
        "cancerModel = get_peft_model(cancerModel, peft_config)\n",
        "diabetesModel = get_peft_model(diabetesModel, peft_config)\n",
        "\n",
        "\n",
        "# pretrain_data_collator = DataCollatorForPreTraining(tokenizer)\n",
        "# mlm_data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)"
      ],
      "metadata": {
        "id": "tDSBLE7Katm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# diabetesModel.bert.encoder.layer[0].attention.self.query.weight\n",
        "\n",
        "\n",
        "# Parameter containing:\n",
        "# tensor([[-0.0164,  0.0261, -0.0263,  ...,  0.0154,  0.0768,  0.0548],\n",
        "#         [-0.0326,  0.0346, -0.0423,  ..., -0.0527,  0.1393,  0.0078],\n",
        "#         [ 0.0105,  0.0334,  0.0109,  ..., -0.0279,  0.0258, -0.0468],\n",
        "#         ...,\n",
        "#         [-0.0085,  0.0514,  0.0555,  ...,  0.0282,  0.0543, -0.0541],\n",
        "#         [-0.0198,  0.0944,  0.0617,  ..., -0.1042,  0.0601,  0.0470],\n",
        "#         [ 0.0015, -0.0952,  0.0099,  ..., -0.0191, -0.0508, -0.0085]])"
      ],
      "metadata": {
        "id": "q6u-7wcbwIx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_fine_tune_trainer = Trainer(\n",
        "        model = cancerModel,\n",
        "        train_dataset = tokenized_labelled_cancer,\n",
        "        eval_dataset = tokenized_labelled_eval_cancer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        # dataset_text_field = \"text\",\n",
        "        # max_seq_length = 128,\n",
        "        # dataset_num_proc = 2,\n",
        "        # peft_config = peft_config,\n",
        "        # packing = False,\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 5,\n",
        "            num_train_epochs = 10,\n",
        "            learning_rate = 1e-4,\n",
        "            fp16 = False, #not is_bfloat16_supported(),\n",
        "            bf16 = True, #is_bfloat16_supported(),\n",
        "            logging_steps = 1,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed= 890,\n",
        "            output_dir= \"cancer_outputs\",\n",
        "            run_name = \"cancer_est_run_fine_tune_seq\",\n",
        "        ),\n",
        "    )\n",
        "\n",
        "diabetes_fine_tune_trainer = Trainer(\n",
        "        model = diabetesModel,\n",
        "        train_dataset = tokenized_labelled_diabetes,\n",
        "        eval_dataset = tokenized_labelled_eval_diabetes,\n",
        "        compute_metrics=compute_metrics,\n",
        "        # dataset_text_field = \"text\",\n",
        "        # max_seq_length = 128,\n",
        "        # dataset_num_proc = 2,\n",
        "        # peft_config = peft_config,\n",
        "        # packing = False,\n",
        "        args = TrainingArguments(\n",
        "            per_device_train_batch_size = 2,\n",
        "            gradient_accumulation_steps = 4,\n",
        "            warmup_steps = 5,\n",
        "            num_train_epochs = 10,\n",
        "            learning_rate = 1e-4,\n",
        "            fp16 = False, #not is_bfloat16_supported(),\n",
        "            bf16 = True, #is_bfloat16_supported(),\n",
        "            logging_steps = 1,\n",
        "            optim = \"adamw_8bit\",\n",
        "            weight_decay = 0.01,\n",
        "            lr_scheduler_type = \"linear\",\n",
        "            seed= 890,\n",
        "            output_dir= \"diabetes_outputs\",\n",
        "            run_name = \"diabetes_est_run_fine_tune_seq\",\n",
        "        ),\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "Yb2E3G5qaxMK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cancer_fine_tune_trainer.train()"
      ],
      "metadata": {
        "id": "twfkgEt7yfSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes_fine_tune_trainer.train()"
      ],
      "metadata": {
        "id": "KCdiV5T87gpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuS4xCIYIJ3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}