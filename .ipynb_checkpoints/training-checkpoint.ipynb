{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S1QyNPRV3bG"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft bitsandbytes\n",
    "!pip install wandb\n",
    "!pip install datasets\n",
    "!pip install pyarrow==15.0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0NFw_kvU0P6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import gc\n",
    "from datasets import Dataset\n",
    "\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForMaskedLM,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForPreTraining,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from peft import get_peft_model\n",
    "\n",
    "import wandb\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "woyIppUA0aoG"
   },
   "outputs": [],
   "source": [
    "use_dtype = torch.bfloat16\n",
    "\n",
    "### Classification head to go on LLM\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, output_dim, dtype=use_dtype)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "### Combine LLM and Classification Head\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, llama_model, classifier):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.llama_model = llama_model\n",
    "        self.classifier = classifier\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None):\n",
    "        with torch.no_grad():  # Freeze pretrained model weights\n",
    "            outputs = self.llama_model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "\n",
    "        ### Pass the output representation to linear layer\n",
    "        logits = self.classifier(outputs.logits.mean(axis=1).to(device, dtype=use_dtype))\n",
    "        return logits\n",
    "\n",
    "### Tokenize the datasets\n",
    "def tokenize_data(data):\n",
    "    return tokenizer(data['text'], truncation=True, padding='max_length', max_length=max_seq_length)\n",
    "\n",
    "### Save weights and biases (wandb)\n",
    "def save_wandb(model_name):\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"lea\")\n",
    "    model_artifact = wandb.Artifact(model_name, type='model')\n",
    "    torch.save(model.state_dict(), f'{model_name}.pth')\n",
    "    model_artifact.add_file(f'{model_name}.pth')\n",
    "    wandb.log_artifact(model_artifact)\n",
    "\n",
    "    wandb.log({\"per_device_train_batch_size\": 2,\n",
    "              \"gradient_accumulation_steps\": 4,\n",
    "              \"warmup_steps\": 5,\n",
    "              \"num_train_epochs\": 20,\n",
    "              \"learning_rate\": 1e-4,\n",
    "              \"logging_steps\": 1,\n",
    "              \"optim\": \"adamw_8bit\",\n",
    "              \"weight_decay\": 0.01,\n",
    "              \"lr_scheduler_type\": \"linear\",\n",
    "              \"seed\": 890,\n",
    "              \"output_dir\": \"outputs\",\n",
    "              \"train_test_split_random_state\": 42\n",
    "              })\n",
    "\n",
    "def get_pretrained_model(model_name, max_seq_length, dtype, load_in_4bit):\n",
    "    return FastLanguageModel.from_pretrained(\n",
    "        model_name = model_name,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "      )\n",
    "\n",
    "### Read weights and biases (wandb)\n",
    "def read_model_wandb(model, model_name):\n",
    "    wandb.login()\n",
    "    wandb.init(project=\"clinical_nlp\")\n",
    "    artifact = wandb.use_artifact(\"marvinmeng11-foursquare/clinical_nlp/unsloth-llama3-7b-2nd:latest\", type=\"model\")\n",
    "    artifact_dir = artifact.download()\n",
    "    model.load_state_dict(torch.load(f\"{artifact_dir}/{model_name}\"))\n",
    "\n",
    "### Finetine Pretrained LLM\n",
    "def finetune_LLM(lora_model, tokenizer, tokenized_labelled, max_seq_length):\n",
    "    trainer = SFTTrainer(\n",
    "        model = lora_model,\n",
    "        tokenizer = tokenizer,\n",
    "        train_dataset = tokenized_labelled,\n",
    "        dataset_text_field = \"text\",\n",
    "        max_seq_length = max_seq_length,\n",
    "        dataset_num_proc = 2,\n",
    "        packing = False,\n",
    "        args = TrainingArguments(\n",
    "            per_device_train_batch_size = 2,\n",
    "            gradient_accumulation_steps = 4,\n",
    "            warmup_steps = 5,\n",
    "            num_train_epochs = 20,\n",
    "            learning_rate = 1e-4,\n",
    "            fp16 = not is_bfloat16_supported(),\n",
    "            bf16 = is_bfloat16_supported(),\n",
    "            logging_steps = 1,\n",
    "            optim = \"adamw_8bit\",\n",
    "            weight_decay = 0.01,\n",
    "            lr_scheduler_type = \"linear\",\n",
    "            seed = 890,\n",
    "            output_dir = \"outputs\",\n",
    "        ),\n",
    "    )\n",
    "\n",
    "def confidence_filter(row):\n",
    "    ### Threshold to measure confidence in \"Yes\" or \"No\"\n",
    "    threshold = .2\n",
    "    return row['label'][0] <= threshold or row['label'][0] >= (1 - threshold) or row['label'][1] <= threshold or row['label'][1] >= (1 - threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fm_AdMrYxaA"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rP3Wd8_CuvP4"
   },
   "outputs": [],
   "source": [
    "# ### Separate Test Set and Non-Test Set\n",
    "# base_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/base_data.csv\")\n",
    "\n",
    "# test_set = base_data[base_data[\"test_set\"] == 1]\n",
    "# non_test_set = base_data[base_data[\"test_set\"] == 0]\n",
    "\n",
    "# test_set.to_csv(\"test_data.csv\", index=False)\n",
    "# non_test_set.to_csv(\"train_data.csv\", index=False)\n",
    "\n",
    "### Load in Data\n",
    "train_data = pd.read_csv(\"drive/MyDrive/Colab Notebooks/health_data/train_data.csv\")\n",
    "\n",
    "### Split Labelled and Unlabelled\n",
    "train_data_labelled = train_data[~train_data[\"has_cancer\"].isnull()]\n",
    "train_data_unlabelled = train_data[train_data[\"has_cancer\"].isnull()]\n",
    "\n",
    "### Combine Cancer and Diabetes Labels\n",
    "train_data_labelled['label'] = train_data_labelled.apply(lambda row: [row['has_cancer'], row['has_diabetes']], axis=1)\n",
    "train_data_labelled = train_data_labelled.drop(['has_cancer', 'has_diabetes', 'test_set', 'patient_identifier'], axis=1)\n",
    "\n",
    "### Create Training and Validation sets\n",
    "train_split_labelled, test_split_labelled = train_test_split(train_data_labelled, test_size=0.3, random_state = 42)\n",
    "\n",
    "\n",
    "### Convert data to Huggingface Datasets\n",
    "labelled_dataset = Dataset.from_pandas(train_split_labelled)\n",
    "unlabelled_dataset = Dataset.from_pandas(train_data_unlabelled)\n",
    "labelled_eval_dataset = Dataset.from_pandas(test_split_labelled)\n",
    "\n",
    "### Pull in LLM and Tokenizer\n",
    "model, tokenizer = get_pretrained_model(\"unsloth/llama-3-8b-bnb-4bit\", 1024, use_dtype, True)\n",
    "\n",
    "### Add LORA Adapters\n",
    "lora_model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    use_gradient_checkpointing = \"unsloth\",\n",
    "    random_state = 890,\n",
    "    use_rslora = True,\n",
    ")\n",
    "\n",
    "### Read in trained models, if applicable\n",
    "# read_model_wandb(\"unsloth-llama3-7b-3rd.pth\")\n",
    "\n",
    "tokenized_labelled = labelled_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_unlabelled = unlabelled_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_labelled_eval = labelled_eval_dataset.map(tokenize_data, batched=True)\n",
    "\n",
    "tokenized_labelled = tokenized_labelled.select_columns(['label', 'input_ids', 'attention_mask'])\n",
    "tokenized_unlabelled = tokenized_unlabelled.select_columns(['input_ids', 'attention_mask'])\n",
    "tokenized_labelled_eval = tokenized_labelled_eval.select_columns(['label','input_ids', 'attention_mask'])\n",
    "\n",
    "tokenized_labelled.set_format(\"torch\")\n",
    "tokenized_unlabelled.set_format(\"torch\")\n",
    "tokenized_labelled_eval.set_format(\"torch\")\n",
    "\n",
    "### Create Dataloaders\n",
    "batch_size = 8\n",
    "test_dataloader = DataLoader(tokenized_unlabelled, batch_size=batch_size, shuffle=False)\n",
    "train_dataloader = DataLoader(tokenized_labelled, batch_size=batch_size, shuffle=True)\n",
    "eval_dataloader = DataLoader(tokenized_labelled_eval, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "### Finetune LLM\n",
    "finetune_LLM(lora_model, tokenizer, tokenized_labelled, max_seq_length)\n",
    "\n",
    "# ### Save Model to wandb\n",
    "# save_wandb('unsloth-llama3-7b-5')\n",
    "input_dim = 128256\n",
    "output_dim = 2\n",
    "classifier = Classifier(input_dim, output_dim)\n",
    "\n",
    "# Instantiate the combined model\n",
    "combined_model = CombinedModel(lora_model, classifier)\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = combined_model.llama_model.device\n",
    "combined_model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(combined_model.classifier.parameters(), lr=1e-6)\n",
    "# criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "combined_model.train()\n",
    "\n",
    "iter_counter = 0\n",
    "n_iterations = 10\n",
    "\n",
    "train_len = len(train_dataloader)\n",
    "\n",
    "### Generate Pseudo-Labels on unlabelled data\n",
    "### Move pseudo-labelled data to training set if score is confident enough\n",
    "### Remove low-confidence pseudo-labels and predict again after training on new data set\n",
    "while(tokenized_unlabelled.num_rows > 0) or (iter_counter < n_iterations):\n",
    "    torch.cuda.empty_cache()\n",
    "    iter_counter += 1\n",
    "    print(\"Iteration: \", iter_counter)\n",
    "    print(\"Starting Training\")\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for batch in train_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                logits = torch.sigmoid(combined_model(input_ids, attention_mask=attention_mask))\n",
    "                loss = criterion(logits.float(), labels.float())\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss / train_len}\")\n",
    "\n",
    "\n",
    "    print(\"Predict Pseudo-Labels\")\n",
    "    all_predictions = []\n",
    "    counter = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            # Move batch to GPU if available\n",
    "            batch = {k: v.to('cuda') for k, v in batch.items()}\n",
    "\n",
    "            # Make predictions\n",
    "            outputs = combined_model(**batch)\n",
    "            predictions = torch.sigmoid(outputs.float())\n",
    "\n",
    "            # Collect predictions\n",
    "\n",
    "            all_predictions += [i for i in predictions.cpu().float().numpy()]\n",
    "\n",
    "    print(\"Update Training Set\")\n",
    "    pseudo_labelled_dataset = tokenized_unlabelled.add_column('label', all_predictions)\n",
    "    confident = pseudo_labelled_dataset.filter(confidence_filter)\n",
    "\n",
    "    ### Reset unlabelled data\n",
    "    tokenized_unlabelled = pseudo_labelled_dataset.filter(lambda x: not confidence_filter(x)).remove_columns(['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcOS5upXYt2b"
   },
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_pzWdCx_8Oty"
   },
   "outputs": [],
   "source": [
    "### Evaluate\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    mean_squared_error,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "outputs = combined_model(tokenized_labelled_eval['input_ids'], tokenized_labelled_eval['attention_mask'])\n",
    "predictions = torch.sigmoid(outputs)\n",
    "cancer_probabilities = [x[0].item() for x in predictions]\n",
    "diabetes_probabilities = [x[1].item() for x in predictions]\n",
    "\n",
    "cancer_predictions = [round(x) for x in cancer_probabilities]\n",
    "diabetes_predictions = [round(x) for x in diabetes_probabilities]\n",
    "\n",
    "cancer_labels = [x[0].item() for x in tokenized_labelled_eval['label']]\n",
    "diabetes_labels = [x[1].item() for x in tokenized_labelled_eval['label']]\n",
    "\n",
    "cancer_labels = np.array(cancer_labels)\n",
    "cancer_predictions = np.array(cancer_predictions)\n",
    "cancer_probabilities = np.array(cancer_probabilities)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(cancer_labels, cancer_predictions)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(cancer_labels, cancer_predictions)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(cancer_labels, cancer_predictions)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(cancer_labels, cancer_predictions)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(cancer_labels, cancer_probabilities)\n",
    "\n",
    "# Area Under the Receiver Operating Characteristic Curve (AUROC)\n",
    "auroc = roc_auc_score(cancer_labels, cancer_probabilities)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"AUROC: {auroc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONWHZaj4-0pL"
   },
   "outputs": [],
   "source": [
    "### Diabetes Metrics\n",
    "diabetes_labels = np.array(diabetes_labels)\n",
    "diabetes_predictions = np.array(diabetes_predictions)\n",
    "diabetes_probabilities = np.array(diabetes_probabilities)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = accuracy_score(diabetes_labels, diabetes_predictions)\n",
    "\n",
    "# Precision\n",
    "precision = precision_score(diabetes_labels, diabetes_predictions)\n",
    "\n",
    "# Recall\n",
    "recall = recall_score(diabetes_labels, diabetes_predictions)\n",
    "\n",
    "# F1-Score\n",
    "f1 = f1_score(diabetes_labels, diabetes_predictions)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(diabetes_labels, diabetes_probabilities)\n",
    "\n",
    "# AUROC\n",
    "auroc = roc_auc_score(diabetes_labels, diabetes_probabilities)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"AUROC: {auroc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S98LK6yGY0J8"
   },
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yi5dmrkr_X-n"
   },
   "outputs": [],
   "source": [
    "### Predict outputs for test set\n",
    "\n",
    "final_test_set = pd.read_csv(\"drive/MyDrive/Colab Notebooks/data/test_data.csv\")\n",
    "final_test_dataset = Dataset.from_pandas(final_test_set)\n",
    "tokenized_final_test = final_test_dataset.map(tokenize_data, batched=True)\n",
    "tokenized_final_test = tokenized_final_test.select_columns(['patient_identifier', 'input_ids', 'attention_mask'])\n",
    "tokenized_final_test.set_format(\"torch\")\n",
    "\n",
    "### Create Dataloaders\n",
    "batch_size = 16\n",
    "final_test_dataloader = DataLoader(tokenized_final_test, batch_size=batch_size, shuffle=False)\n",
    "all_predictions = []\n",
    "ids = []\n",
    "for batch in final_test_dataloader:\n",
    "    new_batch = {'input_ids': batch['input_ids'].to('cuda'), 'attention_mask': batch['attention_mask'].to('cuda')}\n",
    "\n",
    "    # Make predictions\n",
    "    outputs = combined_model(**new_batch)\n",
    "    predictions = torch.sigmoid(outputs.float())\n",
    "\n",
    "    # Collect predictions\n",
    "    all_predictions += [i for i in predictions.cpu().detach().numpy()]\n",
    "    ids += batch['patient_identifier']\n",
    "\n",
    "\n",
    "\n",
    "ids = [x.item() for x in ids]\n",
    "cancer_probabilities = [x[0].item() for x in all_predictions]\n",
    "diabetes_probabilities = [x[1].item() for x in all_predictions]\n",
    "\n",
    "cancer_predictions = [round(x) for x in cancer_probabilities]\n",
    "diabetes_predictions = [round(x) for x in diabetes_probabilities]\n",
    "\n",
    "final_df = pd.DataFrame({'patient_identifier': ids, 'cancer_probability': cancer_probabilities, 'diabetes_probability': diabetes_probabilities ,'has_cancer': cancer_predictions, 'has_diabetes': diabetes_predictions})\n",
    "final_df.to_csv(\"predictions.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
